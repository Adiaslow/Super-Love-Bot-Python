# SUPER LOVE BOT!!!# Adam Murray# 10/09/2020 - present# Python 3.7.6########################################################## A program designed to automate the process of:        ##                                                       ## 1: Generating content                                 ##   1) Mine the web for information on love             ##   2) Input that data into a neural network            ##   3) Stylize the output                               ##   4) Post stylized output to social media platforms   ##                                                       ## 2: Optimizing outreach                                ##   1) Post on a schedule                               ##   2) Use most popular hashtags                        ##   3) Like popular relavant posts and posts of friends ##   4) Follow back new followers                        ##   5) Reply to comments and messages                   ####################################################################################################################                      Libararies                       ##########################################################from bs4 import BeautifulSoupimport argparseimport refrom urllib.request import urlopen, Requestimport osimport os.pathimport tensorflow as tfimport numpy as npimport sysimport reimport randomimport timeimport matplotlib.pyplot as pltfrom PIL import Image, ImageDraw, ImageFont, ImageColorfrom random import randintfrom keras.models import Sequentialfrom keras.layers import Dense, Activationfrom keras.layers import LSTMfrom keras.optimizers import RMSpropfrom keras.callbacks import ModelCheckpointfrom keras.callbacks import ReduceLROnPlateaufrom keras.callbacks import LambdaCallbackfrom datetime import date, datetime    ##########################################################                       Scrapers                        ##########################################################def goodQuotesScraper():        maxQuotes = 347828    maxPages = maxQuotes / 30        ap = argparse.ArgumentParser(description='Scrape quotes from Goodreads.com')        ap.add_argument("-t", "--tag",                    required=False, type=str, default=None,                    help="tag (topic/theme) of quotes to scrape")    ap.add_argument("-p", "--max_pages",                    required=False, type=int, default=maxPages,                    help="maximum number of webpages to scrape")    ap.add_argument("-q", "--max_quotes",                    required=False, type=int, default=maxQuotes,                    help="maximum number of quotes to scrape")        args = vars(ap.parse_args())        args = vars(ap.parse_args())        def download_goodreads_quotes(tag, max_pages=maxPages, max_quotes=maxQuotes):            def download_quotes_from_page(tag, page):                def compile_url(tag, page):                return f'https://www.goodreads.com/quotes/tag/{tag}?page={page}'                def get_soup(url):                response = urlopen(Request(url))                return BeautifulSoup(response, 'html.parser')                def extract_quotes_elements_from_soup(soup):                elements_quotes = soup.find_all("div", {"class": "quote mediumText"})                return elements_quotes                def extract_quote_dict(quote_element):                    def extract_quote(quote_element):                    try:                        quote = quote_element.find('div', {'class': 'quoteText'}).get_text("|", strip=True)                        # first element is always the quote                        quote = quote.split('|')[0]                        quote = re.sub('^“', '', quote)                        quote = re.sub('”\s?$', '', quote)                        return quote                    except:                        return None                    def extract_author(quote_element):                    try:                        author = quote_element.find('span', {'class': 'authorOrTitle'}).get_text()                        author = author.strip()                        author = author.rstrip(',')                        return author                    except:                        return None                    def extract_source(quote_element):                    try:                        source = quote_element.find('a', {'class': 'authorOrTitle'}).get_text()                        return source                    except:                        return None                    def extract_tags(quote_element):                    try:                        tags = quote_element.find('div', {'class': 'greyText smallText left'}).get_text(strip=True)                        tags = re.sub('^tags:', '', tags)                        tags = tags.split(',')                        return tags                    except:                        return None                    def extract_likes(quote_element):                    try:                        likes = quote_element.find('a', {'class': 'smallText', 'title': 'View this quote'}).get_text(strip=True)                        likes = re.sub('likes$', '', likes)                        likes = likes.strip()                        return int(likes)                    except:                        return None                    quote_data = {'quote': extract_quote(quote_element),                              'author': extract_author(quote_element),                              'source': extract_source(quote_element),                              'likes': extract_likes(quote_element),                              'tags': extract_tags(quote_element)}                    return quote_data                url = compile_url(tag, page)            print(f'Retrieving {url}...')            soup = get_soup(url)            quote_elements = extract_quotes_elements_from_soup(soup)                return [extract_quote_dict(e) for e in quote_elements]            def download_all_pages(tag, max_pages, max_quotes):            results = []            p = 1            while p <= max_pages:                res = download_quotes_from_page(tag, p)                if len(res) == 0:                    print(f'No results found on page {p}.\nTerminating search.')                    return results                    results = results + res                    if len(results) >= max_quotes:                    print(f'Hit quote maximum ({max_quotes}) on page {p}.\nDiscontinuing search.')                    return results[0:max_quotes]                else:                    p += 1                return results            return download_all_pages(tag, max_pages, max_quotes)        def recreate_quote(dict):        return f'"{dict.get("quote")}"'        def save_quotes(quote_data, tag, maxQuotes):        save_path = '/Users/Adam/Desktop/Super Love Bot/Scrapings'        name_of_file = 'goodreads' + '-' + str(maxQuotes) + '-' + tag + '-' + 'quotes'        completeName = os.path.join(save_path, name_of_file+".txt")                        print('saving file')        with open(completeName, 'w', encoding='utf-8') as f:            quotes = [recreate_quote(q) for q in quote_data]            for q in quotes:                f.write(q + '\n')    if __name__ == '__main__':        tag = args['tag'] if args['tag'] != None else input('Provide tag to search quotes for: ')        mp = args['max_pages']        mq = args['max_quotes']        result = download_goodreads_quotes(tag, max_pages=mp, max_quotes=mq)        save_quotes(result, tag, maxQuotes)##########################################################                       Analyzers                        ##########################################################        def singleWordCounter():    dataFile = '/Users/Adam/Desktop/Super Love Bot/Scrapings/goodreads-50-love-quotes.txt'        with open(dataFile,"r") as scrapings:        text = scrapings.read()    cleanText = re.sub(r'[^\w]', ' ', text)        lowerCaseText = cleanText.lower()        wordCount = len(cleanText.split())         wordList = lowerCaseText.split()        wordToFind = input("What word would you like to count?: ")        # print(wordToFind)        wordCounter = 0        for i in range(0, wordCount, 1):        # print(wordList[i])        if wordToFind == wordList[i]:            wordCounter += 1        else:            continue        print("")    print("\"", wordToFind, "\" shows up", wordCounter, "times.")    def wordAndCharCounter():    dataFile = '/Users/Adam/Desktop/Super Love Bot/Scrapings/goodreads-50-love-quotes.txt'        with open(dataFile,"r") as scrapings:        text = scrapings.read()    cleanText = re.sub(r'[^\w]', ' ', text)        lowerCaseText = cleanText.lower()        wordCount = len(cleanText.split())        charCount = len(text)        uniqueCharCount = len(sorted(list(set(text))))    print("File contains", wordCount, "words and", charCount, "characters with", uniqueCharCount, "unique characters.")        ##########################################################                    Random Genorators                  ##########################################################def randomSentenceGen1():        dataFile = '/Users/Adam/Desktop/Super Love Bot/Scrapings/goodreads-50-love-quotes.txt'        with open(dataFile,"r") as scrapings:        text = scrapings.read()    cleanText = re.sub(r'[^\w]', ' ', text)        lowerCaseText = cleanText.lower()        wordCount = len(cleanText.split())         wordList = lowerCaseText.split()        # print(wordCount)        # print(cleanText)        desiredNumOfWords = randint(5, 13)        print(desiredNumOfWords)        sentence = ''        for i in range(0, desiredNumOfWords, 1):        if i != desiredNumOfWords - 1:            sentence += wordList[randint(0, wordCount)] + " "        else:            sentence += wordList[randint(0, wordCount)]        sentence += "."    sentence = sentence[0].upper() + sentence[1:]        print(sentence)        now = str(datetime.now())    fileName = re.sub(r'[^\w]', ' ', now)    fileSuffix = str(".txt")    fileName = now + fileSuffix                # print(fileName)        sentenceFile = open(fileName, "w")        def haikuGen1():    dataFile = '/Users/Adam/Desktop/Super Love Bot/Scrapings/goodreads-50-love-quotes.txt'        with open(dataFile,"r") as scrapings:        text = scrapings.read()    cleanText = re.sub(r'[^\w]', ' ', text)        lowerCaseText = cleanText.lower()        wordList = lowerCaseText.split()        print(wordList)        oneLetterList = []    twoLetterList = []    threeLetterList = []    fourLetterList = []    fiveLetterList = []    sixLetterList = []    sevenLetterList = []        for i in range(0, len(wordList) - 1, 1):        if len(wordList[i]) == 1:            oneLetterList += wordList[i]        if len(wordList[i]) == 2:            twoLetterList += wordList[i]        if len(wordList[i]) == 3:            threeLetterList += wordList[i]        if len(wordList[i]) == 4:            fourLetterList += wordList[i]        if len(wordList[i]) == 5:            fiveLetterList += wordList[i]        if len(wordList[i]) == 6:            sixLetterList += wordList[i]        if len(wordList[i]) == 7:            sevenLetterList += wordList[i]        # print(threeLetterList)    ##########################################################                    Neural Networks                    ##########################################################        def kerasNeuralNetwork():        dataFile = '/Users/Adam/Desktop/Projects/Super Love Bot/Scrapings/goodreads-5-love-quotes.txt'        fileToWriteToName = time.strftime("%Y%m%d-%H%M%S")        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'        with open(dataFile, 'r') as file:        text = file.read().lower()    print('text length:', len(text))        chars = sorted(list(set(text))) # getting all unique chars    print('total chars:', len(chars))        char_indices = dict((c, i) for i, c in enumerate(chars))    indices_char = dict((i, c) for i, c in enumerate(chars))        maxlen = 100    step = 3    sentences = []    next_chars = []    for i in range(0, len(text) - maxlen, step):        sentences.append(text[i: i + maxlen])        next_chars.append(text[i + maxlen])    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)    for i, sentence in enumerate(sentences):        for t, char in enumerate(sentence):            x[i, t, char_indices[char]] = 1        y[i, char_indices[next_chars[i]]] = 1            model = Sequential()    model.add(LSTM(128, input_shape=(maxlen, len(chars))))    model.add(Dense(len(chars)))    model.add(Activation('softmax'))    optimizer = RMSprop(lr=0.01)    model.compile(loss='categorical_crossentropy', optimizer=optimizer)        def sample(preds, temperature=1.0):        # helper function to sample an index from a probability array        preds = np.asarray(preds).astype('float64')        preds = np.log(preds) / temperature        exp_preds = np.exp(preds)        preds = exp_preds / np.sum(exp_preds)        probas = np.random.multinomial(1, preds, 1)        return np.argmax(probas)    def on_epoch_end(epoch, logs):        # Function invoked at end of each epoch. Prints generated text.        print()        print('----- Generating text after Epoch: %d' % epoch)            start_index = random.randint(0, len(text) - maxlen - 1)        for diversity in [0.2, 0.5, 1.0, 1.2]:            print('----- diversity:', diversity)                generated = ''            sentence = text[start_index: start_index + maxlen]            generated += sentence            print('----- Generating with seed: "' + sentence + '"')            sys.stdout.write(generated)                                        for i in range(400):                x_pred = np.zeros((1, maxlen, len(chars)))                for t, char in enumerate(sentence):                    x_pred[0, t, char_indices[char]] = 1.                    preds = model.predict(x_pred, verbose=0)[0]                next_index = sample(preds, diversity)                next_char = indices_char[next_index]                    generated += next_char                sentence = sentence[1:] + next_char                    sys.stdout.write(next_char)                sys.stdout.flush()                                save_path = '/Users/Adam/Desktop/Projects/Super Love Bot/RNN Outputs'                name_of_file = fileToWriteToName                completeName = os.path.join(save_path, name_of_file+".txt")                with open(completeName, 'w', encoding='utf-8') as f:                    f.write(sentence + '.' + '\n')                            print()                print_callback = LambdaCallback(on_epoch_end=on_epoch_end)                filepath = "weights4.hdf5"    checkpoint = ModelCheckpoint(filepath, monitor='loss',                                 verbose=1, save_best_only=True,                                 mode='min')        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,                                  patience=1, min_lr=0.001)    callbacks = [print_callback, checkpoint, reduce_lr]        model.fit(x, y, batch_size=128, epochs=100, callbacks=callbacks)    ##########################################################                    Image Generator                    ##########################################################    def instagramPostGen():    # genorates random hex    number_of_colors = 2        BGHexColor = ("#"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))    print(BGHexColor)        textHexColor = ("#"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))    print(textHexColor)        colors = [BGHexColor, textHexColor]        # plots colors    """    for i in range(number_of_colors):        plt.scatter(random.randint(0, 10), random.randint(0,10), c=colors[i], s=200)        plt.show()    """    # converts hex to RGB    BGRGBColor = ImageColor.getrgb(BGHexColor)    print(BGRGBColor)        textRGBColor = ImageColor.getrgb(textHexColor)    print(textRGBColor)        # recieves message    msg = "Wove coul\nnove to\nthe wis\nson the\nlove cave, er\nlove."        # creates image        whiteText = True        W, H = (1080, 1080)        if whiteText == False:        textRGBColor = textRGBColor            elif whiteText == True:        textRGBColor = (256, 256, 256)            img = Image.new('RGB', (W, H), color = (BGRGBColor))         fnt = ImageFont.truetype('/Library/Fonts/Hack-Regular.ttf', 150)    d = ImageDraw.Draw(img)    w, h = d.textsize(msg)    d.text((100, 100), msg, font=fnt, fill=(textRGBColor))        fileToWriteToName = time.strftime("%Y%m%d-%H%M%S")    save_path = '/Users/Adam/Desktop/Super Love Bot/Posts'    name_of_file = fileToWriteToName    completeName = os.path.join(save_path, name_of_file+".png")         img.save(completeName)    def fractalGen():    def mandelbrot(n_rows, n_columns, iterations, cx, cy):        # generator        x_cor = np.linspace(-2, 2, n_rows)        y_cor = np.linspace(-2, 2, n_columns)        x_len = len(x_cor)        y_len = len(y_cor)        output = np.zeros((x_len,y_len))        c = complex(cx, cy)        for i in range(x_len):            for j in range(y_len):                z = complex(x_cor[i], y_cor[j])                count = 0                for k in range(iterations):                    z = (z * z) + c                    count = count + 1                    if (abs(z) > 4):                        break                output[i,j] = count            print(int((i/x_len)*100),"% completed")            print(output)        plt.imshow(output.T, cmap='afmhot')                fileToWriteToName = time.strftime("%Y%m%d-%H%M%S")        save_path = '/Users/Adam/Desktop/Super Love Bot/Fractals'        name_of_file = fileToWriteToName        completeName = os.path.join(save_path, name_of_file+".tiff")            plt.savefig(completeName, dpi = 3840)        plt.axis("off")                plt.show()                """           #randomizer            first1 = randint(-100, 100) / 100    first2 = randint(0, 100) / 100    firstOpRand = randint(0,3)    if firstOpRand == 0:        firstOp = "+"    if firstOpRand == 1:        firstOp = "-"    if firstOpRand == 2:        firstOp = "*"    if firstOpRand == 3:        firstOp = "/"    firstValue = str(first1) + str(firstOp) + str(first2) + 'j'    print(firstValue)    """    mandelbrot(4000, 4000, 300, -0.42+0.7j, -0.42+0.3j)##########################################################                         Menu                          ##########################################################def menu():    while True:        # main menu        print("")        print("What catagory would you like to access?")        print("")        print("1. Scrapers")        print("2. Analyzers")        print("3. Generators")        print("4. Posters")        print("5. Automations")        print("6. Quit")        catPicker = input("Enter number: ")        print("")                # scrapers menu        if catPicker == '1':                        print("What function would you like to access?")            print("")            print("1. Goodreads Scraper")            funcPicker = input("Enter number: ")            print("")                        if funcPicker == '1':                goodQuotesScraper()                 # anylizers menu        if catPicker == '2':                        print("What function would you like to access?")            print("")            print("1. Single Word Counter")            print("2. Total Word and Character Count")            funcPicker = input("Enter number: ")            print("")                        if funcPicker == '1':                singleWordCounter()                            if funcPicker == '2':                  wordAndCharCounter()                            # generators menu        if catPicker == '3':                        print("What function would you like to access?")            print("")            print("1. Text Gen Reccurent Neural Network")            print("2. Random Sentence Gen 1")            print("3. Haiku Gen 1")            print("4. Instagram Post Gen")            print("5. Fractal Gen")            funcPicker = input("Enter number: ")            print("")                        if funcPicker == '1':                kerasNeuralNetwork()            if funcPicker == '2':                randomSentenceGen1()            if funcPicker == '3':                haikuGen1()            if funcPicker == '4':                instagramPostGen()            if funcPicker == '5':                fractalGen()            # posters menu                                # automation menu                if catPicker == '6':            break                print("")        print("*****************************************")            ##########################################################                         Main                          ##########################################################            def main():        menu()            main()